{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6028ce2c-5da4-4c08-bbb5-b40f68a2347d",
   "metadata": {},
   "source": [
    "# Decision Trees\n",
    "In this notebook, we'll implement the decision tree algorithm, and use it in the example of identifying edible mushrooms as in the course graded lab.  \n",
    "The code here are based on my own implementations in the graded lab, organized and rewritten to be more succinct and clear."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "314ecf8e-fc66-4541-9809-87dd75971bec",
   "metadata": {},
   "source": [
    "## Tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "id": "1027d619-4892-44be-b11d-9925415adba3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c307c8c-6341-4987-8315-9826f72f3534",
   "metadata": {},
   "source": [
    "## Dataset\n",
    "\n",
    "The dataset for this task is as follows:\n",
    "\n",
    "|                                                     | Cap Color | Stalk Shape | Solitary | Edible |\n",
    "|:---------------------------------------------------:|:---------:|:-----------:|:--------:|:------:|\n",
    "| <img src=\"images/0.png\" alt=\"drawing\" width=\"50\"/> |   Brown   |   Tapering  |    Yes   |    1   |\n",
    "| <img src=\"images/1.png\" alt=\"drawing\" width=\"50\"/> |   Brown   |  Enlarging  |    Yes   |    1   |\n",
    "| <img src=\"images/2.png\" alt=\"drawing\" width=\"50\"/> |   Brown   |  Enlarging  |    No    |    0   |\n",
    "| <img src=\"images/3.png\" alt=\"drawing\" width=\"50\"/> |   Brown   |  Enlarging  |    No    |    0   |\n",
    "| <img src=\"images/4.png\" alt=\"drawing\" width=\"50\"/> |   Brown   |   Tapering  |    Yes   |    1   |\n",
    "| <img src=\"images/5.png\" alt=\"drawing\" width=\"50\"/> |    Red    |   Tapering  |    Yes   |    0   |\n",
    "| <img src=\"images/6.png\" alt=\"drawing\" width=\"50\"/> |    Red    |  Enlarging  |    No    |    0   |\n",
    "| <img src=\"images/7.png\" alt=\"drawing\" width=\"50\"/> |   Brown   |  Enlarging  |    Yes   |    1   |\n",
    "| <img src=\"images/8.png\" alt=\"drawing\" width=\"50\"/> |    Red    |   Tapering  |    No    |    1   |\n",
    "| <img src=\"images/9.png\" alt=\"drawing\" width=\"50\"/> |   Brown   |  Enlarging  |    No    |    0   |\n",
    "\n",
    "\n",
    "-  You have 10 examples of mushrooms. For each example, you have\n",
    "    - Three features\n",
    "        - Cap Color (`Brown` or `Red`),\n",
    "        - Stalk Shape (`Tapering (as in \\/)` or `Enlarging (as in /\\)`), and\n",
    "        - Solitary (`Yes` or `No`)\n",
    "    - Label\n",
    "        - Edible (`1` indicating yes or `0` indicating poisonous)\n",
    "\n",
    "### One hot encoded dataset\n",
    "We can one-hot encoded the features in our dataset:  \n",
    "\n",
    "- `X_train` contains three features for each example \n",
    "    - Brown Color (A value of `1` indicates \"Brown\" cap color and `0` indicates \"Red\" cap color)\n",
    "    - Tapering Shape (A value of `1` indicates \"Tapering Stalk Shape\" and `0` indicates \"Enlarging\" stalk shape)\n",
    "    - Solitary  (A value of `1` indicates \"Yes\" and `0` indicates \"No\")\n",
    "- `y_train` is whether the mushroom is edible \n",
    "    - `y = 1` indicates edible\n",
    "    - `y = 0` indicates poisonous"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "id": "4d0dcac1-4dd8-4eb1-a84f-926d02a1d53a",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = np.array([[1,1,1],[1,0,1],[1,0,0],[1,0,0],[1,1,1],[0,1,1],[0,0,0],[1,0,1],[0,1,0],[1,0,0]])\n",
    "y_train = np.array([1,1,0,0,1,0,0,1,1,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "id": "5424111e-affd-41a8-9fda-e31730912a3d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The shape of X_train is: (10, 3)\n",
      "The shape of y_train is:  (10,)\n",
      "Number of training examples (m): 10\n"
     ]
    }
   ],
   "source": [
    "print ('The shape of X_train is:', X_train.shape)\n",
    "print ('The shape of y_train is: ', y_train.shape)\n",
    "print ('Number of training examples (m):', len(X_train))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f988a5e-d834-4e79-8188-a1649d579476",
   "metadata": {},
   "source": [
    "## Implementation\n",
    "We'll follow the guidelines of the decision tree algorithm to implement our decision tree.\n",
    "- Start with all examples at the root node\n",
    "- Calculate information gain for all possible features, and pick one with the hightest information gain\n",
    "- Split dataset according to selected feature, and create left and right branches of the tree\n",
    "- Keep repeating splitting process until stopping criteria is met, here we'll use these two criterias:\n",
    "    - When a node is 100% one class\n",
    "    - When splitting a node will result in the tree exceeding maximum depth"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92b3f96f-9408-4583-a322-d499692ca2df",
   "metadata": {},
   "source": [
    "Our implementation of the decision tree will consist of the following functions:\n",
    "- `compute_entropy`: computes entropy at a given node\n",
    "- `split_dataset`: takes in the data at a node and a feature to split on and splits it into left and right branches\n",
    "- `compute_information_gain`: computes information gain of a split\n",
    "- `get_best_feature`: get the best feature to split the node\n",
    "- `build_tree_recursive`: builds the decision tree recursively"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "id": "39af5f31-81b7-4dbb-8c45-8c9c9a933253",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_entropy(y):\n",
    "    \"\"\"\n",
    "    Computes the entropy at a given node\n",
    "    \n",
    "    Args:\n",
    "       y (ndarray): labels of the examples at the node\n",
    "       \n",
    "    Returns:\n",
    "        entropy (float): Entropy at that node       \n",
    "    \"\"\"\n",
    "    if len(y) == 0:  # If there's no example in the node, entropy defined to be 0\n",
    "        return 0\n",
    "        \n",
    "    p1 = np.sum(y) / len(y)\n",
    "    \n",
    "    if p1 == 0 or p1 == 1:  # If p1 = 0 or p0 = 0, entropy is defined to be 0\n",
    "        return 0\n",
    "\n",
    "    entropy = - p1 * np.log2(p1) - (1 - p1) * np.log2(1 - p1)\n",
    "    \n",
    "    return entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "id": "9c8a5f56-3e49-4983-a995-c3ef413bf08d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entropy at root node:  1.0\n"
     ]
    }
   ],
   "source": [
    "# Compute entropy at the root node (i.e. with all examples)\n",
    "# Since we have 5 edible and 5 non-edible mushrooms, the entropy should be 1\"\n",
    "\n",
    "print(\"Entropy at root node: \", compute_entropy(y_train)) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71313f83-6776-46ff-850d-f26c1f7f969b",
   "metadata": {},
   "source": [
    "**Expected Output**:\n",
    "<table>\n",
    "  <tr>\n",
    "    <td> <b>Entropy at root node:<b> 1.0 </td> \n",
    "  </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "id": "28b7c5cd-40ab-4994-beab-03c094795653",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_dataset(X, node_indices, feature):\n",
    "    \"\"\"\n",
    "    Splits the data at the given node into left and right branches\n",
    "    \n",
    "    Args:\n",
    "        X (ndarray):             Data matrix of shape(n_samples, n_features)\n",
    "        node_indices (list):     List containing the active indices. I.e, the samples being considered at this step.\n",
    "        feature (int):           Index of feature to split on\n",
    "    \n",
    "    Returns:\n",
    "        left_indices (list):     Indices with feature value == 1\n",
    "        right_indices (list):    Indices with feature value == 0\n",
    "    \"\"\"\n",
    "    msk_arr = X[node_indices, feature] == 1  # Create a mask array for samples with feature value == 1\n",
    "    left_indices = node_indices[msk_arr]\n",
    "    right_indices = node_indices[~msk_arr]\n",
    "    \n",
    "    return left_indices, right_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "id": "541b34e4-d72d-4822-95d6-a02fbaa67764",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CASE 1:\n",
      "Left indices:  [0 1 2 3 4 7 9]\n",
      "Right indices:  [5 6 8]\n",
      "CASE 2:\n",
      "Left indices:  [0 2 4]\n",
      "Right indices:  [6 8]\n"
     ]
    }
   ],
   "source": [
    "# Case 1\n",
    "root_indices = np.array([0, 1, 2, 3, 4, 5, 6, 7, 8, 9])\n",
    "feature = 0\n",
    "\n",
    "left_indices, right_indices = split_dataset(X_train, root_indices, feature)\n",
    "\n",
    "print(\"CASE 1:\")\n",
    "print(\"Left indices: \", left_indices)\n",
    "print(\"Right indices: \", right_indices)\n",
    "\n",
    "# Case 2\n",
    "root_indices_subset = np.array([0, 2, 4, 6, 8])\n",
    "left_indices, right_indices = split_dataset(X_train, root_indices_subset, feature)\n",
    "\n",
    "print(\"CASE 2:\")\n",
    "print(\"Left indices: \", left_indices)\n",
    "print(\"Right indices: \", right_indices)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "593c661a-c35c-4593-b51b-2e233f335948",
   "metadata": {},
   "source": [
    "**Expected Output**:\n",
    "```\n",
    "CASE 1:\n",
    "Left indices:  [0, 1, 2, 3, 4, 7, 9]\n",
    "Right indices:  [5, 6, 8]\n",
    "\n",
    "CASE 2:\n",
    "Left indices:  [0, 2, 4]\n",
    "Right indices:  [6, 8]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "id": "793d0ff4-5b46-48fb-9800-d4ebb7560e43",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_information_gain(X, y, node_indices, feature): \n",
    "    \"\"\"\n",
    "    Compute the information gain of splitting the node on a given feature\n",
    "    \n",
    "    Args:\n",
    "        X (ndarray):            Data matrix of shape(n_samples, n_features)\n",
    "        y (array like):         List or ndarray with n_samples containing the target variable\n",
    "        node_indices (ndarray): List containing the active indices. I.e, the samples being considered in this step.\n",
    "        feature (int):          Index of feature to split on\n",
    "   \n",
    "    Returns:\n",
    "        info_gain (float):      Information gain\n",
    "    \"\"\"\n",
    "    left_indices, right_indices = split_dataset(X, node_indices, feature)\n",
    "    w_left = len(left_indices) / len(node_indices)\n",
    "    w_right = 1 - w_left\n",
    "    info_gain = compute_entropy(y[node_indices]) - (w_left * compute_entropy(y[left_indices]) + \n",
    "                                                    w_right * compute_entropy(y[right_indices]))\n",
    "    return info_gain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "id": "55e3dad3-be71-4039-b0ad-5d12069cf7e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Information Gain from splitting the root on brown cap:  0.034851554559677034\n",
      "Information Gain from splitting the root on tapering stalk shape:  0.12451124978365313\n",
      "Information Gain from splitting the root on solitary:  0.2780719051126377\n"
     ]
    }
   ],
   "source": [
    "info_gain0 = compute_information_gain(X_train, y_train, root_indices, feature=0)\n",
    "print(\"Information Gain from splitting the root on brown cap: \", info_gain0)\n",
    "\n",
    "info_gain1 = compute_information_gain(X_train, y_train, root_indices, feature=1)\n",
    "print(\"Information Gain from splitting the root on tapering stalk shape: \", info_gain1)\n",
    "\n",
    "info_gain2 = compute_information_gain(X_train, y_train, root_indices, feature=2)\n",
    "print(\"Information Gain from splitting the root on solitary: \", info_gain2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff1d71ab-5e29-411e-9f6b-84fbbfa4f4c3",
   "metadata": {},
   "source": [
    "**Expected Output**:\n",
    "```\n",
    "Information Gain from splitting the root on brown cap:  0.034851554559677034\n",
    "Information Gain from splitting the root on tapering stalk shape:  0.12451124978365313\n",
    "Information Gain from splitting the root on solitary:  0.2780719051126377\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "id": "1c4fb682-e181-4470-83c3-65ffcb9e4bb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_best_feature(X, y, node_indices):   \n",
    "    \"\"\"\n",
    "    Returns the optimal feature and threshold value\n",
    "    to split the node data \n",
    "    \n",
    "    Args:\n",
    "        X (ndarray):            Data matrix of shape(n_samples, n_features)\n",
    "        y (array like):         list or ndarray with n_samples containing the target variable\n",
    "        node_indices (ndarray): List containing the active indices. I.e, the samples being considered in this step.\n",
    "\n",
    "    Returns:\n",
    "        best_feature (int):     The index of the best feature to split\n",
    "    \"\"\"    \n",
    "    best_feature = -1\n",
    "    root_entropy = compute_entropy(y[node_indices])\n",
    "    \n",
    "    if root_entropy == 0:\n",
    "        return best_feature\n",
    "\n",
    "    # Vectorize the function compute_information_gain over the feature argument\n",
    "    vectorized_func = np.vectorize(lambda feature: compute_information_gain(X, y, node_indices, feature))\n",
    "\n",
    "    features = np.arange(X.shape[1])\n",
    "    info_gains = vectorized_func(features)\n",
    "    best_feature = np.argmax(info_gains)\n",
    "\n",
    "    return best_feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "id": "d551e5c3-4786-4667-b463-c2d828e2624a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best feature to split on: 2\n"
     ]
    }
   ],
   "source": [
    "best_feature = get_best_feature(X_train, y_train, root_indices)\n",
    "print(\"Best feature to split on: %d\" % best_feature)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98006957-3732-407a-9f05-5efa001daa2d",
   "metadata": {},
   "source": [
    "Expect that the best feature to split on at the root node is feature 2 (\"Solitary\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7451e9fc-41e9-4bb7-aba5-19335d03810f",
   "metadata": {},
   "source": [
    "### Recursive function to build the decision tree\n",
    "Here we implement by ourselves the recursive function to build the tree. If the one of the two following criteria is reached, stop splitting the tree and print the leaf nodes:\n",
    "- When a node is 100% one class\n",
    "- When splitting a node will result in the tree exceeding maximum depth\n",
    "\n",
    "We print the tree with some formatting to make the tree structure more clear."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "id": "524341ff-06a2-4006-96bd-0cb8e7196f7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_tree_recursive(X, y, node_indices, branch_name, max_depth, current_depth, feature_names, indent=\"\"):\n",
    "    \"\"\"\n",
    "    Build a tree using the recursive algorithm that split the dataset into 2 subgroups at each node.\n",
    "    This function just prints the tree.\n",
    "    \n",
    "    Args:\n",
    "        X (ndarray):            Data matrix of shape(n_samples, n_features)\n",
    "        y (array like):         list or ndarray with n_samples containing the target variable\n",
    "        node_indices (ndarray): List containing the active indices. I.e, the samples being considered in this step.\n",
    "        branch_name (string):   Name of the branch. ['Root', 'Left', 'Right']\n",
    "        max_depth (int):        Max depth of the resulting tree. \n",
    "        current_depth (int):    Current depth. Parameter used during recursive call.\n",
    "        dash (string):        Space indented for printing the tree\n",
    "        feature_names (list):   List of names of the features\n",
    "        \n",
    "    \"\"\" \n",
    "    # Print the current branch and depth\n",
    "    print(\"{}Depth {}, {}:\".format(indent, current_depth, branch_name))\n",
    "    \n",
    "    # First check if the tree has reached maximum depth\n",
    "    if current_depth == max_depth:\n",
    "        # Reached maximum depth, stop splitting\n",
    "        # Print the leaf node\n",
    "        print(\"{}  [Leaf] Reached maximum depth. Samples: {}\".format(indent, node_indices))\n",
    "        return None\n",
    "\n",
    "    # Then check if the node is at 100% purity\n",
    "    best_feature = get_best_feature(X, y, node_indices)\n",
    "    if best_feature == -1:\n",
    "        # Node is at 100% purity, stop splitting\n",
    "        # Print the leaf node\n",
    "        print(\"{}  [Leaf] Node is at 100% purity. Samples: {}\".format(indent, node_indices))\n",
    "        return None\n",
    "\n",
    "    print(\"{}  Split on feature {} [{}]\".format(indent, best_feature, feature_names[best_feature]))\n",
    "    left_indices, right_indices = split_dataset(X, node_indices, best_feature)\n",
    "          \n",
    "    build_tree_recursive(X, y, left_indices, \"Left\", max_depth, current_depth + 1, feature_names, indent + \"  \")\n",
    "    build_tree_recursive(X, y, right_indices, \"Right\", max_depth, current_depth + 1, feature_names, indent + \"  \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "id": "d8fa6401-8519-4601-af5e-b579f489bf6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "root_indices = np.arange(X_train.shape[0])\n",
    "feature_names = [\"Brown cap color\", \"Tapering Stalk Shape\", \"Solitary\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "id": "226b8d46-c47e-4455-a0e2-ee4b35b16037",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Depth 0, Root:\n",
      "  Split on feature 2 [Solitary]\n",
      "  Depth 1, Left:\n",
      "    Split on feature 0 [Brown cap color]\n",
      "    Depth 2, Left:\n",
      "      [Leaf] Reached maximum depth. Samples: [0 1 4 7]\n",
      "    Depth 2, Right:\n",
      "      [Leaf] Reached maximum depth. Samples: [5]\n",
      "  Depth 1, Right:\n",
      "    Split on feature 1 [Tapering Stalk Shape]\n",
      "    Depth 2, Left:\n",
      "      [Leaf] Reached maximum depth. Samples: [8]\n",
      "    Depth 2, Right:\n",
      "      [Leaf] Reached maximum depth. Samples: [2 3 6 9]\n"
     ]
    }
   ],
   "source": [
    "build_tree_recursive(X_train, y_train, root_indices, \"Root\", 2, 0, feature_names, \"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "id": "27625273-142f-4d95-ad21-46001b9bbeb4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Depth 0, Root:\n",
      "  Split on feature 2 [Solitary]\n",
      "  Depth 1, Left:\n",
      "    Split on feature 0 [Brown cap color]\n",
      "    Depth 2, Left:\n",
      "      [Leaf] Node is at 100% purity. Samples: [0 1 4 7]\n",
      "    Depth 2, Right:\n",
      "      [Leaf] Node is at 100% purity. Samples: [5]\n",
      "  Depth 1, Right:\n",
      "    Split on feature 1 [Tapering Stalk Shape]\n",
      "    Depth 2, Left:\n",
      "      [Leaf] Node is at 100% purity. Samples: [8]\n",
      "    Depth 2, Right:\n",
      "      [Leaf] Node is at 100% purity. Samples: [2 3 6 9]\n"
     ]
    }
   ],
   "source": [
    "build_tree_recursive(X_train, y_train, root_indices, \"Root\", 4, 0, feature_names, \"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d32d6ac0-0678-4bde-bb41-43d6168a6222",
   "metadata": {},
   "source": [
    "We can see that when we set the maximum depth to 2, the tree stops splitting because the maximum depth is reached; when we set the maximum depth to 4, the tree stops splitting because the nodes are at 100% purity. The splitting result is consistent with the result we got in the lab."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bfd4f09-80aa-436c-bab0-62072be5c4d6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
