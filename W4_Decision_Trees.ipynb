{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6028ce2c-5da4-4c08-bbb5-b40f68a2347d",
   "metadata": {},
   "source": [
    "# Decision Trees\n",
    "In this notebook, we'll implement the decision tree algorithm, and use it in the example of identifying edible mushrooms as in the course graded lab.  \n",
    "The code here are based on my own implementations in the graded lab, organized and rewritten to be more concise and clear."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "314ecf8e-fc66-4541-9809-87dd75971bec",
   "metadata": {},
   "source": [
    "## Tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1027d619-4892-44be-b11d-9925415adba3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c307c8c-6341-4987-8315-9826f72f3534",
   "metadata": {},
   "source": [
    "## Dataset\n",
    "\n",
    "The dataset for this task is as follows:\n",
    "\n",
    "|                                                     | Cap Color | Stalk Shape | Solitary | Edible |\n",
    "|:---------------------------------------------------:|:---------:|:-----------:|:--------:|:------:|\n",
    "| <img src=\"images/0.png\" alt=\"drawing\" width=\"50\"/> |   Brown   |   Tapering  |    Yes   |    1   |\n",
    "| <img src=\"images/1.png\" alt=\"drawing\" width=\"50\"/> |   Brown   |  Enlarging  |    Yes   |    1   |\n",
    "| <img src=\"images/2.png\" alt=\"drawing\" width=\"50\"/> |   Brown   |  Enlarging  |    No    |    0   |\n",
    "| <img src=\"images/3.png\" alt=\"drawing\" width=\"50\"/> |   Brown   |  Enlarging  |    No    |    0   |\n",
    "| <img src=\"images/4.png\" alt=\"drawing\" width=\"50\"/> |   Brown   |   Tapering  |    Yes   |    1   |\n",
    "| <img src=\"images/5.png\" alt=\"drawing\" width=\"50\"/> |    Red    |   Tapering  |    Yes   |    0   |\n",
    "| <img src=\"images/6.png\" alt=\"drawing\" width=\"50\"/> |    Red    |  Enlarging  |    No    |    0   |\n",
    "| <img src=\"images/7.png\" alt=\"drawing\" width=\"50\"/> |   Brown   |  Enlarging  |    Yes   |    1   |\n",
    "| <img src=\"images/8.png\" alt=\"drawing\" width=\"50\"/> |    Red    |   Tapering  |    No    |    1   |\n",
    "| <img src=\"images/9.png\" alt=\"drawing\" width=\"50\"/> |   Brown   |  Enlarging  |    No    |    0   |\n",
    "\n",
    "\n",
    "-  You have 10 examples of mushrooms. For each example, you have\n",
    "    - Three features\n",
    "        - Cap Color (`Brown` or `Red`),\n",
    "        - Stalk Shape (`Tapering (as in \\/)` or `Enlarging (as in /\\)`), and\n",
    "        - Solitary (`Yes` or `No`)\n",
    "    - Label\n",
    "        - Edible (`1` indicating yes or `0` indicating poisonous)\n",
    "\n",
    "### One hot encoded dataset\n",
    "We can one-hot encoded the features in our dataset:  \n",
    "\n",
    "- `X_train` contains three features for each example \n",
    "    - Brown Color (A value of `1` indicates \"Brown\" cap color and `0` indicates \"Red\" cap color)\n",
    "    - Tapering Shape (A value of `1` indicates \"Tapering Stalk Shape\" and `0` indicates \"Enlarging\" stalk shape)\n",
    "    - Solitary  (A value of `1` indicates \"Yes\" and `0` indicates \"No\")\n",
    "- `y_train` is whether the mushroom is edible \n",
    "    - `y = 1` indicates edible\n",
    "    - `y = 0` indicates poisonous"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4d0dcac1-4dd8-4eb1-a84f-926d02a1d53a",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = np.array([[1,1,1],[1,0,1],[1,0,0],[1,0,0],[1,1,1],[0,1,1],[0,0,0],[1,0,1],[0,1,0],[1,0,0]])\n",
    "y_train = np.array([1,1,0,0,1,0,0,1,1,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5424111e-affd-41a8-9fda-e31730912a3d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The shape of X_train is: (10, 3)\n",
      "The shape of y_train is:  (10,)\n",
      "Number of training examples (m): 10\n"
     ]
    }
   ],
   "source": [
    "print ('The shape of X_train is:', X_train.shape)\n",
    "print ('The shape of y_train is: ', y_train.shape)\n",
    "print ('Number of training examples (m):', len(X_train))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f988a5e-d834-4e79-8188-a1649d579476",
   "metadata": {},
   "source": [
    "## Implementation\n",
    "We'll follow the guidelines of the decision tree algorithm to implement our decision tree.\n",
    "- Start with all examples at the root node\n",
    "- Calculate information gain for all possible features, and pick one with the hightest information gain\n",
    "- Split dataset according to selected feature, and create left and right branches of the tree\n",
    "- Keep repeating splitting process until stopping criteria is met, here we'll use these two criterias:\n",
    "    - When a node is 100% one class\n",
    "    - When splitting a node will result in the tree exceeding maximum depth"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92b3f96f-9408-4583-a322-d499692ca2df",
   "metadata": {},
   "source": [
    "Our implementation of the decision tree will consist of the following functions:\n",
    "- `compute_entropy`: computes entropy at a given node\n",
    "- `split_dataset`: takes in the data at a node and a feature to split on and splits it into left and right branches\n",
    "- `compute_information_gain`: computes information gain of a split\n",
    "- `get_best_feature`: get the best feature to split the node\n",
    "- `build_tree_recursive`: builds the decision tree recursively"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "39af5f31-81b7-4dbb-8c45-8c9c9a933253",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_entropy(y):\n",
    "    \"\"\"\n",
    "    Computes the entropy at a given node\n",
    "    \n",
    "    Args:\n",
    "       y (ndarray): labels of the examples at the node\n",
    "       \n",
    "    Returns:\n",
    "        entropy (float): Entropy at that node       \n",
    "    \"\"\"\n",
    "    if len(y) == 0:  # If there's no example in the node, entropy defined to be 0\n",
    "        return 0\n",
    "        \n",
    "    p1 = np.sum(y) / len(y)\n",
    "    \n",
    "    if p1 == 0 or p1 == 1:  # If p1 = 0 or p0 = 0, entropy is defined to be 0\n",
    "        return 0\n",
    "\n",
    "    entropy = - p1 * np.log2(p1) - (1 - p1) * np.log2(1 - p1)\n",
    "    \n",
    "    return entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9c8a5f56-3e49-4983-a995-c3ef413bf08d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entropy at root node:  1.0\n"
     ]
    }
   ],
   "source": [
    "# Compute entropy at the root node (i.e. with all examples)\n",
    "# Since we have 5 edible and 5 non-edible mushrooms, the entropy should be 1\"\n",
    "\n",
    "print(\"Entropy at root node: \", compute_entropy(y_train)) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71313f83-6776-46ff-850d-f26c1f7f969b",
   "metadata": {},
   "source": [
    "**Expected Output**:\n",
    "<table>\n",
    "  <tr>\n",
    "    <td> <b>Entropy at root node:<b> 1.0 </td> \n",
    "  </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "28b7c5cd-40ab-4994-beab-03c094795653",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_dataset(X, node_indices, feature):\n",
    "    \"\"\"\n",
    "    Splits the data at the given node into left and right branches\n",
    "    \n",
    "    Args:\n",
    "        X (ndarray):             Data matrix of shape(n_samples, n_features)\n",
    "        node_indices (list):     List containing the active indices. I.e, the samples being considered at this step.\n",
    "        feature (int):           Index of feature to split on\n",
    "    \n",
    "    Returns:\n",
    "        left_indices (list):     Indices with feature value == 1\n",
    "        right_indices (list):    Indices with feature value == 0\n",
    "    \"\"\"\n",
    "    msk_arr = X[node_indices, feature] == 1  # Create a mask array for samples with feature value == 1\n",
    "    left_indices = node_indices[msk_arr]\n",
    "    right_indices = node_indices[~msk_arr]\n",
    "    \n",
    "    return left_indices, right_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "541b34e4-d72d-4822-95d6-a02fbaa67764",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CASE 1:\n",
      "Left indices:  [0 1 2 3 4 7 9]\n",
      "Right indices:  [5 6 8]\n",
      "CASE 2:\n",
      "Left indices:  [0 2 4]\n",
      "Right indices:  [6 8]\n"
     ]
    }
   ],
   "source": [
    "# Case 1\n",
    "root_indices = np.array([0, 1, 2, 3, 4, 5, 6, 7, 8, 9])\n",
    "feature = 0\n",
    "\n",
    "left_indices, right_indices = split_dataset(X_train, root_indices, feature)\n",
    "\n",
    "print(\"CASE 1:\")\n",
    "print(\"Left indices: \", left_indices)\n",
    "print(\"Right indices: \", right_indices)\n",
    "\n",
    "# Case 2\n",
    "root_indices_subset = np.array([0, 2, 4, 6, 8])\n",
    "left_indices, right_indices = split_dataset(X_train, root_indices_subset, feature)\n",
    "\n",
    "print(\"CASE 2:\")\n",
    "print(\"Left indices: \", left_indices)\n",
    "print(\"Right indices: \", right_indices)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "593c661a-c35c-4593-b51b-2e233f335948",
   "metadata": {},
   "source": [
    "**Expected Output**:\n",
    "```\n",
    "CASE 1:\n",
    "Left indices:  [0, 1, 2, 3, 4, 7, 9]\n",
    "Right indices:  [5, 6, 8]\n",
    "\n",
    "CASE 2:\n",
    "Left indices:  [0, 2, 4]\n",
    "Right indices:  [6, 8]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "793d0ff4-5b46-48fb-9800-d4ebb7560e43",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_information_gain(X, y, node_indices, feature): \n",
    "    \"\"\"\n",
    "    Compute the information gain of splitting the node on a given feature\n",
    "    \n",
    "    Args:\n",
    "        X (ndarray):            Data matrix of shape(n_samples, n_features)\n",
    "        y (array like):         List or ndarray with n_samples containing the target variable\n",
    "        node_indices (ndarray): List containing the active indices. I.e, the samples being considered in this step.\n",
    "        feature (int):          Index of feature to split on\n",
    "   \n",
    "    Returns:\n",
    "        info_gain (float):      Information gain\n",
    "    \"\"\"\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fcdf89f-5dd8-4f21-92ac-aa231ad24df4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
